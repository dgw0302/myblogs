---
title: 分布式
categories:
  - 分布式or设计模式
tags:
  - 分布式
abbrlink: 56288
date:  2022-07-25=0 
cover :  https://ts1.cn.mm.bing.net/th/id/R-C.4bfff15bb0fac753cfa24089fd8ffc7c?rik=Ax6G2Xay8nRVtA&riu=http%3a%2f%2fwww.topgoer.cn%2fuploads%2fgolangxiuyang%2fimages%2f129-CAP1.jpeg&ehk=8sYpr1beQZZ6NuUWUQgWhMwmYJcUnBCWHaxYrjjS%2bIU%3d&risl=&pid=ImgRaw&r=0
---







# 分布式全局唯一id

## UUID

采用UUID虽然可以生成

但是是无序的，Mysql底层是B+数，利用主键构建索引，在叶子结点是有序分布的，如果新生成的主键是无序的，如果插入就得把原来的数据往后移动，









当插入的下一条记录的 ID 是递增的时候，比如插入 30 时，数据库只需要把它追加到后面就好了。但是如果插入的数据是无序的，比如 ID 是 13，那么数据库就要查找 13 应该插入的位置，再挪动 13 后面的数据，这就造成了多余的数据移动的开销。





![image-20220720114244212](../../images/%E5%88%86%E5%B8%83%E5%BC%8F/image-20220720114244212.png)



**UUID 不能作为 ID 的另一个原因是它不具备业务含义**





## Snowflake 算法





Snowflake 的核心思想是将 64bit 的二进制数字分成若干部分，每一部分都存储有特定含义的数据，比如说时间戳、机器 ID、序列号等等，最终生成全局唯一的有序 ID。它的标准算法是这样的：



![image-20220720114814374](../../images/%E5%88%86%E5%B8%83%E5%BC%8F/image-20220720114814374.png)







从上面这张图中我们可以看到，41 位的时间戳大概可以支撑 pow(2,41)/1000/60/60/24/365 年，约等于 69 年，对于一个系统是足够了。



如果你的系统部署在多个机房，那么 10 位的机器 ID 可以继续划分为 2～3 位的 IDC 标示（可以支撑 4 个或者 8 个 IDC 机房）和 7～8 位的机器 ID（支持 128-256 台机器）；12 位的序列号代表着每个节点每毫秒最多可以生成 4096 的 ID。



> 
>
> 不同公司也会依据自身业务的特点对 Snowflake 算法做一些改造，比如说减少序列号的位数增加机器 ID 的位数以支持单 IDC 更多的机器，也可以在其中加入业务 ID 字段来区分不同的业务。**比方说我现在使用的发号器的组成规则就是：**1 位兼容位恒为 0 + 41 位时间信息 + 6 位 IDC 信息（支持 64 个 IDC）+ 6 位业务信息（支持 64 个业务）+ 10 位自增信息（每毫秒支持 1024 个号）





我选择这个组成规则，主要是因为我在单机房只部署一个发号器的节点，并且使用 KeepAlive 保证可用性。业务信息指的是项目中哪个业务模块使用，比如用户模块生成的 ID，内容模块生成的 ID，把它加入进来，一是希望不同业务发出来的 ID 可以不同，二是因为在出现问题时可以反解 ID，知道是哪一个业务发出来的 ID。







**那么了解了 Snowflake 算法的原理之后，我们如何把它工程化，来为业务生成全局唯一的 ID 呢？一般来说我们会有两种算法的实现方式：**









> **一种是嵌入到业务代码里，也就是分布在业务服务器中。**这种方案的好处是业务代码在使用的时候不需要跨网络调用，性能上会好一些，但是就需要更多的机器 ID 位数来支持更多的业务服务器。另外，由于业务服务器的数量很多，我们很难保证机器 ID 的唯一性，所以就需要引入 ZooKeeper 等分布式一致性组件来保证每次机器重启时都能获得唯一的机器 ID。
>
> 
>
> 
>
> **另外一个部署方式是作为独立的服务部署，这也就是我们常说的发号器服务。**业务在使用发号器的时候就需要多一次的网络调用，但是内网的调用对于性能的损耗有限，却可以减少机器 ID 的位数，如果发号器以主备方式部署，同时运行的只有一个发号器，那么机器 ID 可以省略，这样可以留更多的位数给最后的自增信息位。即使需要机器 ID，因为发号器部署实例数有限，那么就可以把机器 ID 写在发号器的配置文件里，这样即可以保证机器 ID 唯一性，也无需引入第三方组件了。**微博和美图都是使用独立服务的方式来部署发号器的，性能上单实例单 CPU 可以达到两万每秒。**







# Ｎginx





## 正向代理

正向代理服务器位于客户端和服务器之间，为了从服务器获取数据，客户端要向代理服务器发送一个请求，并指定目标服务器，代理服务器将目标服务器返回的数据转交给客户端。这里客户端需要要进行一些正向代理的设置的。

正向代理中被代理的是客户端的请求

## 反向代理

反向代理，客户端对代理是无感知的，客户端不需要任何配置就可以访问，客户端将请求发送到反向代理服务器，由反向代理服务器去选择目标服务器获取数据后，在返回给客户端，此时反向代理服务器和目标服务器对外就是一个服务器，暴露的是代理服务器地址，隐藏了真实服务器IP地址。



## 负载均衡



**1. 轮询 （round-robin）**

轮询为负载均衡中较为基础也较为简单的算法，它不需要配置额外参数。假设配置文件中共有 台服务器，该算法遍历服务器节点列表，并按节点次序每轮选择一台服务器处理请求。当所有节点均被调用过一次后，该算法将从第一个节点开始重新一轮遍历。

**特点**：由于该算法中每个请求按时间顺序逐一分配到不同的服务器处理，因此适用于服务器性能相近的集群情况，其中每个服务器承载相同的负载。但对于服务器性能不同的集群而言，该算法容易引发资源分配不合理等问题。

**2、加权轮询**

为了避免普通轮询带来的弊端，加权轮询应运而生。在加权轮询中，每个服务器会有各自的 `weight`。一般情况下，`weight` 的值越大意味着该服务器的性能越好，可以承载更多的请求。该算法中，客户端的请求按权值比例分配，当一个请求到达时，优先为其分配权值最大的服务器。

**特点**：加权轮询可以应用于服务器性能不等的集群中，使资源分配更加合理化。

Nginx 加权轮询源码可见：**ngx_http_upstream_round_robin.c**([https://github.com/nginx/nginx/blob/master/src/http/ngx_http_upstream_round_robin.c](https://link.zhihu.com/?target=https%3A//github.com/nginx/nginx/blob/master/src/http/ngx_http_upstream_round_robin.c))，源码分析可参考：**关于轮询策略原理的自我理解**([https://blog.csdn.net/BlacksunAcheron/article/details/84439302](https://link.zhihu.com/?target=https%3A//blog.csdn.net/BlacksunAcheron/article/details/84439302))。其核心思想是，遍历各服务器节点，并计算节点权值，计算规则为`current_weight`与其对应的`effective_weight`之和，每轮遍历中选出权值最大的节点作为最优服务器节点。其中`effective_weight`会在算法的执行过程中随资源情况和响应情况而改变。较为核心的部分如下：

```text
for (peer = rrp->peers->peer, i = 0;
  peer; 	/* peer 为当前遍历的服务器结点*/
  peer = peer->next, i++)
{
  ...
    
  /* 每轮遍历会更新 peer 当前的权值*/
  peer->current_weight += peer->effective_weight;

  ...
    
  /* best 为当前服务器中的最优节点，即本轮中选中的服务器节点*/
  if (best == NULL || peer->current_weight > best->current_weight) {
    best = peer;
    p = i;
  }
  
  ...
}
```

**3. IP 哈希（IP hash）**

`ip_hash` 依据发出请求的客户端 IP 的 hash 值来分配服务器，该算法可以保证同 IP 发出的请求映射到同一服务器，或者具有相同 hash 值的不同 IP 映射到同一服务器。

**特点**：该算法在一定程度上解决了集群部署环境下 Session 不共享的问题。

> “Session 不共享问题是说，假设用户已经登录过，此时发出的请求被分配到了 A 服务器，但 A 服务器突然宕机，用户的请求则会被转发到 B 服务器。但由于 Session 不共享，B 无法直接读取用户的登录信息来继续执行其他操作。

实际应用中，我们可以利用 `ip_hash`，将一部分 IP 下的请求转发到运行新版本服务的服务器，另一部分转发到旧版本服务器上，实现灰度发布。再者，如遇到文件过大导致请求超时的情况，也可以利用 `ip_hash` 进行文件的分片上传，它可以保证同客户端发出的文件切片转发到同一服务器，利于其接收切片以及后续的文件合并操作。

**4、其他算法**

- URL hash
  `url_hash` 是根据请求的 URL 的 hash 值来分配服务器。该算法的特点是，相同 URL 的请求会分配给固定的服务器，当存在缓存的时候，效率一般较高。然而 Nginx 默认不支持这种负载均衡算法，需要依赖第三方库。
- 最小连接数（Least Connections）
  假设共有 台服务器，当有新的请求出现时，遍历服务器节点列表并选取其中连接数最小的一台服务器来响应当前请求。连接数可以理解为当前处理的请求数。



# CDN





CDN（Content Delivery Network/Content Distribution Network，内容分发网络）。 简单来说，CDN 就是将静态的资源分发到，位于多个地理位置机房中的服务器上，因此它 能很好地解决数据就近访问的问题，也就加快了静态资源的访问速度。







# 一致性Hash算法



**小林coding:**

https://xiaolincoding.com/os/8_network_system/hash.html











# 分布式事务



https://xiaomi-info.github.io/2020/01/02/distributed-transaction/

## TCC











# 接口的幂等性



或者说换一种问法：怎样防止用户重复下单



https://mp.weixin.qq.com/s/JLNhuEgZm98fKKR57p4gaQ



## 前言

`接口幂等性`问题，对于开发人员来说，是一个跟语言无关的公共问题。本文分享了一些解决这类问题非常实用的办法，绝大部分内容我在项目中实践过的，给有需要的小伙伴一个参考。

不知道你有没有遇到过这些场景：

1. 有时我们在填写某些`form表单`时，保存按钮不小心快速点了两次，表中竟然产生了两条重复的数据，只是id不一样。
2. 我们在项目中为了解决`接口超时`问题，通常会引入了`重试机制`。第一次请求接口超时了，请求方没能及时获取返回结果（此时有可能已经成功了），为了避免返回错误的结果（这种情况不可能直接返回失败吧？），于是会对该请求重试几次，这样也会产生重复的数据。
3. mq消费者在读取消息时，有时候会读取到`重复消息`（至于什么原因这里先不说，有兴趣的小伙伴，可以找我私聊），如果处理不好，也会产生重复的数据。

没错，这些都是幂等性问题。

`接口幂等性`是指用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生了副作用。

这类问题多发于接口的：

- `insert`操作，这种情况下多次请求，可能会产生重复数据。
- `update`操作，如果只是单纯的更新数据，比如：`update user set status=1 where id=1`，是没有问题的。如果还有计算，比如：`update user set status=status+1 where id=1`，这种情况下多次请求，可能会导致数据错误。









##  insert前先select



## 加悲观锁（※）

数据库表层面

数据库update自带悲观锁

## 加乐观锁（※）



只能用于修改操作

得自己实现

一般在数据库表加个version字段，然后再判断

每次执行修改先查version字段然后判断





##  加唯一索引（※）

数据库表层面



数据库唯一主键的实现主要是利用数据库中主键唯一约束的特性，一般来说唯一主键比较适用于“插入”时的幂等性，其能保证一张表中只能存在一条带该唯一主键的记录。

使用数据库唯一主键完成幂等性时需要注意的是，该主键一般来说并不是使用数据库中自增主键，而是使用分布式 ID 充当主键，这样才能能保证在分布式环境下 ID 的全局唯一性。

- 需要生成全局唯一主键 ID；



这个作为唯一索引的字段一定是全局唯一的



## 建防重表







## 根据状态机



数据库比如订单表加status状态字段



## 加分布式锁（※）

redis分布式锁

这个解决方法与防重Token好像是不一样的

redis 分布式锁机制解决接口幂等性问题。

### 逻辑

1. 用户通过浏览器发起请求，服务端会收集数据，并且生成订单号code作为唯一业务字段。
2. 使用redis的set命令，将该订单code设置到redis中，同时设置超时时间。
3. 判断是否设置成功，如果设置成功，说明是第一次请求，则进行数据操作。
4. 如果设置失败，说明是重复请求，则直接返回成功。

> 需要特别注意的是：分布式锁一定要设置一个合理的过期时间，如果设置过短，无法有效的防止重复请求。如果设置过长，可能会浪费`redis`的存储空间，需要根据实际业务情况而定。

![img](../../images/%E5%88%86%E5%B8%83%E5%BC%8F/195a5543ce45a636b318b3bfdf1422c1.png)





## 防重token（※）



**注意：与上面的redis分布式锁不一样**



##### 方案描述：

针对客户端连续点击或者调用方的超时重试等情况，例如提交订单，此种操作就可以用 Token 的机制实现防止重复提交。简单的说就是调用方在调用接口的时候先向后端请求一个全局 ID（Token），请求的时候携带这个全局 ID 一起请求（Token 最好将其放到 Headers 中），后端需要对这个 Token 作为 Key，用户信息作为 Value 到 Redis 中进行键值内容校验，如果 Key 存在且 Value 匹配就执行删除命令，然后正常执行后面的业务逻辑。如果不存在对应的 Key 或 Value 不匹配就返回重复执行的错误信息，这样来保证幂等操作。

**逻辑：**

1. 第一次请求获取`token`
2. 第二次请求带着这个`token`，完成业务操作。



##### 适用操作：

- 插入操作
- 更新操作
- 删除操作

##### 使用限制：

- 需要生成全局唯一 Token 串；
- 需要使用第三方组件 Redis 进行数据效验；

##### 主要流程：

![图片](../../images/%E5%88%86%E5%B8%83%E5%BC%8F/640.png)

- ① 服务端提供获取 Token 的接口，该 Token 可以是一个序列号，也可以是一个分布式 ID 或者 UUID 串。

- ② 客户端调用接口获取 Token，这时候服务端会生成一个 Token 串。

- ③ 然后将该串存入 Redis 数据库中，以该 Token 作为 Redis 的键（注意设置过期时间）。

- ④ 将 Token 返回到客户端，客户端拿到后应存到表单隐藏域中。

- ⑤ 客户端在执行提交表单时，把 Token 存入到 Headers 中，执行业务请求带上该 Headers。

- ⑥ 服务端接收到请求后从 Headers 中拿到 Token，然后根据 Token 到 Redis 中查找该 key 是否存在。

- ⑦ 服务端根据 Redis 中是否存该 key 进行判断，如果存在就将该 key 删除，然后正常执行业务逻辑。如果不存在就抛异常，返回重复提交的错误信息。

  

> ###### 注意，在并发情况下，执行 Redis 查找数据与删除需要保证原子性，否则很可能在并发下无法保证幂等性。其实现方法可以使用分布式锁或者使用 Lua 表达式来注销查询与删除操作。









## 防止重复提交订单



https://mp.weixin.qq.com/s/yieSqKZbVvpe7R_DhRNVoA
























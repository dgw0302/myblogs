---
title: NIO
categories:
- 操作系统or网络
tags:
- NIO
abbrlink: 56288
date: 2022-06-16 19:54:20
cover : https://pic1.zhimg.com/v2-7d4702713dd772b56baa6efd0c29c04c_r.jpg
---









# Socket

![image-20220614153615462](../../images/NIO/image-20220614153615462-165562506891210.png)



也可以说是操作系统提供的API

![image-20220614153834428](../../images/NIO/image-20220614153834428-165562506891211.png)







  

# BIO与AIO 

## BIO

**BIO Blocking I/，即阻塞的I/O。 **同步阻塞IO模型



  ![image-20220614163059165](../../images/NIO/image-20220614163059165-165562506891213.png)



每一个客户端来连接，就创建一个线程



![image-20220614163254827](../../images/NIO/image-20220614163254827-165562506891212.png)



实际情况下，线程池

![image-20220614163341804](../../images/NIO/image-20220614163341804-165562506891214.png)



带线程池很难处理并发连接



 BIO是面向流的





## AIO

Asynchronous I/O，**异步非阻塞I/O模型**，服务器实现模式为一个有效请求一个线程，客户端的 I/O 请求都是由 OS 先完成了再通知服务器应用启动线程进行处理。 应用场景：适用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK 7 开始支持







## 总结

应用场景：并发连接数不多时采用BIO，因为它编程和调试都非常简单，但如果涉及到高并发的情况，应选择NIO或AIO，更好的建议是采用成熟的网络通信框架Netty。

![图片](../../images/NIO/640-16556250689114.png)

- BIO是一个连接一个线程，**同步阻塞**。
- NIO是一个请求一个线程，**同步非阻塞（IO多路复用）**。
- AIO是一个有效请求一个线程（**异步非阻塞**）。
- NIO比BIO能通过很少的线程数服务更多的用户



#  NIO

**同步阻塞IO模型**

NIO被称为no-blocking io 或者new io都说的通，也称非阻塞IO

**资料**：https://www.zhihu.com/question/29005375

## 区别

![img](../../images/NIO/v2-ef7a975d63fbddbf7bf23014f1c236da_720w-165562506891215.jpg)


可简单认为：**IO是面向流的处理，NIO是面向块(缓冲区)的处理**

- 面向流的I/O 系统**一次一个字节地处理数据**。
- 一个面向块(缓冲区)的I/O系统**以块的形式处理数据**。





  

![img](../../images/NIO/v2-2a1204c317167b975f4abfb3ce5b9452_720w-165562506891216.jpg)



BIO一个线程处理一个客户端连接，而NIO一个线程可以同时处理多个客户端连接

## Reactor模式

https://xiaolincoding.com/os/8_network_system/reactor.html





## 三大组件

![image-20220614205415681](../../images/NIO/image-20220614205415681-165562506891217.png)



NIO主要有**三个核心部分组成**：

- **buffer缓冲区**
- **Channel管道**
- **Selector选择器**





### Selector

Selector(选择器)：选择器用于使用单个线程处理多个通道。因此，它需要较少的线程来处理这些通道。线程之间的切换对于操作系统来说是昂贵的。 因此，为了提高系统效率选择器是有用的。



- Selector选择器就可以比喻成麦当劳的**广播**。
- **一个线程能够管理多个Channel的状态**





### Channel

Channel(通道)：通道是双向的，可读也可写，而流的读写是单向的。无论读写，通道只能和Buffer交互。因为 Buffer，通道可以异步地读写。



Channel，国内大多翻译成“通道”。Channel 和 IO 中的 Stream(流)是差不多一个等级的。只不过 Stream 是单向的，譬如：InputStream, OutputStream，而 Channel 是双向的，既可以用来进行读操作，又可以用来进行写操作。表示 IO 源与目标打开的连接，是双向的，但不能直接访问数据，只能与 Buffer进行交互。通过源码可知，FileChannel 的 read 方法和 write 方法都导致数据复制了两次.



### Buffer

相比于BIO的面试流，NIO是面向缓冲的

并不是直接把数据交给业务程序，是先写向buffer里面，

Buffer是一个对象，它包含一些要写入或者要读出的数据





> 我们的NIO就是**通过Channel管道运输着存储数据的Buffer缓冲区的来实现数据的处理**！
>
> - 要时刻记住：Channel不与数据打交道，它只负责运输数据。与数据打交道的是Buffer缓冲区
>
> - - **Channel-->运输**
>   - **Buffer-->数据**
>
> 相对于传统IO而言，**流是单向的**。对于NIO而言，有了Channel管道这个概念，我们的**读写都是双向**的(铁路上的火车能从广州去北京、自然就能从北京返还到广州)！





## 总结



​          Java NIO 实际上就是多路复用IO。在多路复用 IO 模型中，会有一个线程不断去轮询多个socket 的状态，只有当 socket 真正有读写事件时，才真正调用实际的 IO 读写操作。因为在多路复用 IO 模型中，只需要使用一个线程就可以管理多个socket，系统不需要建立新的进程或者线程，也不必维护这些线程和进程，并且只有在真正有socket 读写事件进行时，才会使用 IO 资源，所以它大大减少了资源占用。在Java NIO 中，是通过 selector.select()去查询每个通道是否有到达事件，如果没有事件，则一直阻塞在那里，因此这种方式会导致用户线程的阻塞。**多路复用 IO 模式，通过一个线程就可以管理多个socket，只有当socket 真正有读写事件发生才会占用资源来进行实际的读写操作。因此，多路复用 IO 比较适合连接数比较多的情况。**



> 
>
> NIO(new io) 主要有三大核心部分：Channel(通道)，Buffer(缓冲区), Selector。传统 IO 基于字节流和字符流进行操作，而NIO 基于 Channel 和 Buffer(缓冲区)进行操作，数据总是从通道读取到缓冲区中，或者从缓冲区写入到通道中。Selector(选择区)用于监听多个通道的事件（比如：连接打开， 数据到达）。因此，单个线程可以监听多个数据通道。









**NIO 的缓冲区**
Java IO 面向流意味着每次从流中读一个或多个字节，直至读取所有字节，它们没有被缓存在任何地方。此外，它不能前后移动流中的数据。如果需要前后移动从流中读取的数据，需要先将它缓 存到一个缓冲区。NIO 的缓冲导向方法不同。数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区中前后移动。这就增加了处理过程中的灵活性。但是，还需要检查是否该缓冲区中包含所 有您需要处理的数据。而且，需确保当更多的数据读入缓冲区时，不要覆盖缓冲区里尚未处理的 数据。



**NIO 的非阻塞**
IO 的各种流是阻塞的。这意味着，当一个线程调用read() 或 write()时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了。 NIO 的非阻塞模式， 使一个线程从某通道发送请求读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取。而不是保持线程阻塞，所以直至数据变的可以读取之前，该线程可以继续做其他的事情。 非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 线程通常将非阻塞 IO 的空闲时间用于在其它通道上执行 IO 操作，所以一个单独的线程现在可以管理多个输入和输出通道（channel）。



### 非阻塞IO还是IO多路复用



资料：https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&mid=2247498988&idx=1&sn=c084fcd9a2fb321e8ee01a5ea7077902&scene=21#wechat_redirect





总结：很模糊，就当它是基于IO多路复用模型



# NIO与传统IO



| **IO** | **NIO**  |
| ------ | -------- |
| 面向流 | 面向缓冲 |
| 阻塞IO | 非阻塞IO |
| 无     | 选择器   |







## **面向流与面向缓冲**

传统IO是面向流的，NIO是面向缓冲区的。 Java IO面向流意味着每次从流中读一个或多个字节，直至读取所有字节，它们没有被缓存在任何地方。此外，它不能前后移动流中的数据。如果需要前后移动从流中读取的数据，需要先将它缓存到一个缓冲区。 Java NIO的缓冲导向方法略有不同。数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区中前后移动。这就增加了处理过程中的灵活性。但是，还需要检查是否该缓冲区中包含所有您需要处理的数据。而且，需确保当更多的数据读入缓冲区时，不要覆盖缓冲区里尚未处理的数据。



**传统Io一个字节一个字节的处理流中的数据。**

**NIO先将数据读取到缓冲区，需要时可在缓冲区中前后移动，有个缓冲区可以更加灵活的处理数据**



## **阻塞与非阻塞IO**

Java IO的各种流是阻塞的。这意味着，当一个线程调用read() 或 write()时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了。Java NIO的非阻塞模式，使一个线程从某通道发送请求读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取，而不是保持线程阻塞，所以直至数据变的可以读取之前，该线程可以继续做其他的事情。 非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 线程通常将非阻塞IO的空闲时间用于在其它通道上执行IO操作，所以一个单独的线程现在可以管理多个输入和输出通道（channel）



**传统IO阻塞**：当一个线程调用read() 或 write()时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了





**NIO阻塞**：如果暂时等不到数据，可以先去干其他事情，提供效率



## **选择器（Selectors）**



 Java NIO的选择器允许一个单独的线程来监视多个输入通道，你可以注册多个通道使用一个选择器，然后使用一个单独的线程来“选择”通道：这些通道里已经有可以处理的输入，或者选择已准备写入的通道。这种选择机制，使得一个单独的线程很容易来管理多个通道。





一个线程可以处理多个网络IO，基于网络IO多路复用







# IO模型

- **阻塞I/O（同步）**
- **非阻塞I/O（同步）**
- **I/O多路复用（同步）**
- 信号驱动I/O（同步）
- 异步I/O（异步）





下面是常用的两种模型

## 阻塞IO

在进程(用户)空间中调用`recvfrom`，其系统调用直到数据包到达且**被复制到应用进程的缓冲区中或者发生错误时才返回**，在此期间**一直等待**。

![img](../../images/NIO/v2-dea93c518e6c3dba3ce2bf452818f366_720w-165562506891218.jpg)



## 非阻塞IO

`recvfrom`从应用层到内核的时候，如果没有数据就**直接返回**一个EWOULDBLOCK错误，一般都对非阻塞I/O模型**进行轮询检查这个状态**，看内核是不是有数据到来。



注：虽然调用内核函数之后不用立即等待数据的返回，但是会不断去轮询来看看数据有没有到来



![img](../../images/NIO/v2-d1ea36da880455688519e6f023c5ec57_720w-165562506891219.jpg)



# IO多路复用

**IO多路复用是同步IO**



强推：https://juejin.cn/post/7036518015462015006

## **同步与异步，阻塞与非阻塞**

> 之前总是分不清这几个区别，这次一网打尽。





**1.**同步不相当于阻塞，异步不相当于非阻塞。

> 同步：调用方法后主动等待结果的返回
>
> 异步：调用方法后不立即等待结果的返回，先去做别的事情，结果返回后，会采用回调方法来处理数据
>
> **同步和异步是相对于操作结果来说，会不会等待结果返回。**
>
> 
>
> 阻塞：结果返回之前，线程不做任何事情，会挂起
>
> 非阻塞：结果返回之前，可以去做一些其他事情
>
> **阻塞和非阻塞是相对于线程是否被阻塞。**





**2.**只是因为同步和阻塞，异步与非阻塞天生就在一起组合，但是其他组合也可以

> 同步非阻塞：比如线程不必一直等待结果的返回，可以先去干其他事情，然后隔一段时间不断地回来看数据有没有返回（采用轮询的模式），很少用。
>
> 
>
> 异步阻塞：有回调方法等待处理返回的数据，不必一直等待结果的返回，但是线程啥也不干。异步体现了，阻塞体现在回调方法一直在等待数据的返回，线程不干其他事情。，这种也很少用。



## select/poll

linux一切皆文件，每个文件都由一个文件描述符（相当于身份证）

![image-20220616211454224](../../images/NIO/image-20220616211454224.png)





把关注读事件的描述符(fd_set*readfds),关注写的文件描述符(fd-set*writefds),等等都往select里面去放

把这些文件描述符集合copy进内核，轮询这些所有的文件描述符，看看有哪些事件发生了，进行标记，标记为可读或者可写，接着再把整个文件描述符集合copy进用户态里面，然后用户态还需要再通过遍历的方式找到可读或者可写的连接（socket或者叫文件），然后再对其进行处理。



**select与poll的区别（区别不大）**

select使用固定长度的BitsMap数组，表示文件描述符集合，而且所支持的文件描述符的个数是有限制，默认最大值1024，只能监听 1024个文件描述符。

poll使用动态数组，以链表形式来组织，突破了select的文件描述符个数限制，当然还会收到系统文件吗描述符限制。



##  epoll（重点）

 

select/poll **需要在用户态与内核态之间拷贝文件描述符集合**，且采用轮询的方式查看文件描述符，这种方式随着并发数上来，性能的损耗会呈指数级增长。

epoll就很好的解决了上面的问题。



**备注：epoll_wait只返回可读可写的文件描述符个数，然后将就绪列表里面的可读或者可写事件复制到其函数参数的event数组，供用户态对应的程序做处理**



> ***第一点*，**epoll 在内核里使用**红黑树来跟踪进程所有待检测的文件描述字**，把需要监控的 socket 通过 `epoll_ctl()` 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删查一般时间复杂度是 `O(logn)`，通过对这棵黑红树进行操作，这样就不需要像 select/poll 每次操作时都传入整个 socket 集合，只需要传入一个待检测的 socket，减少了内核和用户空间大量的数据拷贝和内存分配，一开始epoll_ctl传入当前存在的所有fd集合，不至于以后重复copy,以后有新来的再重新加入。
>
> ***第二点***， epoll 使用事件驱动的机制，内核里**维护了一个链表(双向链表)来记录就绪事件**，当某个 socket 有事件发生时，通过**回调函数**内核会将其加入到这个就绪事件列表中，当用户调用 `e poll_wait()` 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率。
>
> **第三点**，epoll的连接数只受限于机器的大小



![img](../../images/NIO/epoll.png)



**左边select右边epoll的内核源码思路**

![image-20220617114014324](../../images/NIO/image-20220617114014324.png)

> 如上图，每次调用select都要把所有的文件描述符传进内核，当文件描述符某个socket可读或者可写后又把所有文件描述符都从内核copy进用户态
>
> ![image-20220617115335781](../../images/NIO/image-20220617115335781.png)
>
> epoll_create会产生一个epoll实例（或者叫什么句柄），指向了内核eventpoll对象实例（rdlist,等待队列），接下来用epoll_ctl添加或者删除所要监听的socket
>
> epoll_ctl注册要监听的事件类型
>
> epoll_wait则是等待事件的产生,返回的是发生可读或者可写的文件描述符
>
> 
>
> 上面的fds就是要传入所有的socket，而右边的epoll_create只传入待检测的socket













## 总结





在 select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而**epoll事先通过epoll_ctl()来注册一 个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait() 时便得到通知**。(`此处去掉了遍历文件描述符，而是通过监听回调的的机制`。这正是epoll的魅力所在。)

#### 优点：

1. 监视的描述符数量不受限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左 右，具体数目可以cat /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。

   > select的最大缺点就是进程打开的fd是有数量限制的。这对 于连接数量比较大的服务器来说根本不能满足。虽然也可以选择多进程的解决方案( Apache就是这样实现的)，不过虽然linux上面创建进程的代价比较小，但仍旧是不可忽视的，加上进程间数据同步远比不上线程间同步的高效，所以也不是一种完美的方案。

2. epoll是内核空间用一个 红黑树维护所有的fd，epoll_wait 通过回调函数内核会将 I/O 准备好的描述符加入到一个链表中管理，只把就绪的fd用链表复制到用户空间。

3. IO的效率不会随着监视fd的数量的增长而下降。epoll不同于select和poll轮询的方式，而是通过每个fd定义的回调函数来实现的。只有就绪的fd才会执行回调函数。

   > 如果没有大量的idle -connection或者dead-connection，epoll的效率并不会比select/poll高很多，但是当遇到大量的idle- connection，就会发现epoll的效率大大高于select/poll。

> 1. 不用重复传递。我们调用epoll_wait时就相当于以往调用select/poll，但是这时却不用传递socket句柄给内核，因为内核已经在epoll_ctl中拿到了要监控的句柄列表。
>
> 2. 在内核里，一切皆文件。所以，epoll向内核注册了一个文件系统，用于存储上述的被监控socket。当你调用epoll_create时，就会在这个虚拟的epoll文件系统里创建一个file结点。当然这个file不是普通文件，它只服务于epoll。
>
>    epoll在被内核初始化时（操作系统启动），同时会开辟出epoll自己的内核高速cache区，用于安置每一个我们想监控的socket，这些socket会以红黑树的形式保存在内核cache里，以支持快速的查找、插入、删除。这个内核高速cache区，就是建立连续的物理内存页，然后在之上建立slab层，简单的说，就是物理上分配好你想要的size的内存对象，每次使用时都是使用空闲的已分配好的对象。
>
> 3. 极其高效的原因：
>    这是由于我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外，还会**再建立一个list链表，用于存储准备就绪的事件**，当epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效。
>
>    这个准备就绪list链表是怎么维护的呢？
>    当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。
>    上面这句可以看出，epoll的基础就是回调！
>
>    如此，一颗红黑树，一张准备就绪句柄链表，少量的内核cache，就帮我们解决了大并发下的socket处理问题。执行epoll_create时，创建了红黑树和就绪链表，执行epoll_ctl时，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，用于当中断事件来临时向准备就绪链表中插入数据。执行epoll_wait时立刻返回准备就绪链表里的数据即可。

























# 零拷贝

像kafka、Netty、Rocketmq、Nginx、Apache等底层都用到了零拷贝技术，这属于操作系统篇章。



CPU将磁盘数据拷贝到内存



DMA（硬件里面的一个芯片），作用：不需要CPU，由DMA将数据从磁盘拷贝到内存。



![image-20220616173147119](../../images/NIO/image-20220616173147119.png)





传统文件数据传输： 

> - CPU 发出对应的指令给磁盘控制器，然后返回；
> - 磁盘控制器收到指令后，于是就开始准备数据，会把数据放入到磁盘控制器的内部缓冲区中，然后产生一个**中断**；
> - CPU 收到中断信号后，停下手头的工作，接着把磁盘控制器的缓冲区的数据一次一个字节地读进自己的寄存器，然后再把寄存器里的数据写入到内存，而在数据传输的期间 CPU 是无法执行其他任务的。





##  DMA 技术

> **在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务**。

![img](../../images/NIO/%E4%BC%A0%E7%BB%9F%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93.png)



首先，期间共**发生了 4 次用户态与内核态的上下文切换**，因为发生了两次系统调用，一次是 `read()` ，一次是 `write()`，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。



> 其次，还**发生了 4 次数据拷贝**，其中两次是 DMA 的拷贝，另外两次则是通过 CPU 拷贝的，下面说一下这个过程：
>
> - *第一次拷贝*，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 DMA 搬运的。
> - *第二次拷贝*，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的。
> - *第三次拷贝*，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 socket 的缓冲区里，这个过程依然还是由 CPU 搬运的。
> - *第四次拷贝*，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 DMA 搬运的。





## 如何实现零拷贝

零拷贝技术实现的方式通常有 2 种：

- mmap + write
- sendfile

零拷贝就是通过DMA技术以及内存映射减少上面说的上下文切换次数，也就是减少系统调用次数。





### mmap + write

在前面我们知道，`read()` 系统调用的过程中会把内核缓冲区的数据拷贝到用户的缓冲区里，于是为了减少这一步开销，我们可以用 `mmap()` 替换 `read()` 系统调用函数。





**`重点：mmap()` 系统调用函数会直接把内核缓冲区里的数据「映射」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。**接下来应用进程跟操作系统内核「共享」这个缓冲区；

![img](../../images/NIO/mmap%20%252B%20write%20%E9%9B%B6%E6%8B%B7%E8%B4%9D.png)



**详情：小林coding**









反应堆模式

注册感兴趣的事件  -> 扫描是否有感兴趣的事件发生 -> 事件发生后做出相应的处理























































epoll



reactor(小林coding)



 

> 6.19
>
> jvm(重要) + 设计模式 +分布式理论 + java8(lambda + stram)



RPC做了解





 





























































 























































